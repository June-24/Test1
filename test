import tensorflow as tf
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Example sentences
sentence1 = "I love natural language processing"
sentence2 = "Deep learning is fascinating"

# Tokenize the sentences
tokenizer = Tokenizer()
tokenizer.fit_on_texts([sentence1, sentence2])

# Convert sentences to sequences of integers
sequences = tokenizer.texts_to_sequences([sentence1, sentence2])

# Pad sequences to make them of equal length
max_len = max(len(seq) for seq in sequences)
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')

# Define an Embedding layer
vocab_size = len(tokenizer.word_index) + 1  # add 1 for the zero padding
embedding_dim = 20  # dimension of the dense embeddings
embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)

# Get embeddings for the sentences
embeddings = embedding_layer(tf.constant(padded_sequences))

# Print the embeddings
print("Embedding for Sentence 1:")
print(embeddings[0])  # Embedding for the first sentence
print("\nEmbedding for Sentence 2:")
print(embeddings[1])  # Embedding for the second sentence

